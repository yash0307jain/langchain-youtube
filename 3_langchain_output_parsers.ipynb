{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0f48e1",
   "metadata": {},
   "source": [
    "## LangChain Output Parsers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2fc0660c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981f989",
   "metadata": {},
   "source": [
    "#### 1. String Output Parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0895424",
   "metadata": {},
   "source": [
    "`Without Chain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44156d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"List three cities that start with the letter '{letter}'.\")\n",
    "prompt_text = prompt.format(letter=\"P\")\n",
    "\n",
    "# Define the parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build the LCEL (LangChain Expression Language) Chain: Prompt | Model | Parser\n",
    "response = llm.invoke(prompt_text)\n",
    "\n",
    "# Invoke the chain\n",
    "result_str = parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7dbe1578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw String Output\n",
      "Sure! Here are three cities that start with the letter 'P':\n",
      "\n",
      "1. Paris (France)\n",
      "2. Phoenix (United States)\n",
      "3. Pune (India)\n",
      "Type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw String Output\")\n",
    "print(result_str)\n",
    "print(f\"Type: {type(result_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e2856",
   "metadata": {},
   "source": [
    "`With Chain`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0b480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"List three cities that start with the letter '{letter}'.\")\n",
    "\n",
    "# Define the parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Build the LCEL (LangChain Expression Language) Chain: Prompt | Model | Parser\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke the chain\n",
    "result_str = chain.invoke({\"letter\": \"P\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a9ae6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw String Output\n",
      "Here are three cities that start with the letter 'P':\n",
      "\n",
      "1. Paris (France)\n",
      "2. Philadelphia (USA)\n",
      "3. Pune (India)\n",
      "Type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw String Output\")\n",
    "print(result_str)\n",
    "print(f\"Type: {type(result_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3735a9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2. Comma Separated List Output Parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a303a1",
   "metadata": {},
   "source": [
    "`With get_format_instructions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "881cff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Instantiate the specialized parser\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# The key step: get the required formatting instructions!\n",
    "format_instructions = list_parser.get_format_instructions()  # this hold the parser instruction prompt\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4be01691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['item_type'] input_types={} partial_variables={'format_instructions': 'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'item_type'], input_types={}, partial_variables={}, template='List three {item_type}. \\n\\n{format_instructions}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Inject the instructions into the prompt template\n",
    "list_prompt = ChatPromptTemplate.from_template(\"List three {item_type}. \\n\\n{format_instructions}\")\n",
    "\n",
    "# partial - we can fill some values beforehand\n",
    "list_prompt_partial = list_prompt.partial(format_instructions=format_instructions)\n",
    "print(list_prompt_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68da7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LCEL Chain\n",
    "list_chain = list_prompt_partial | llm | list_parser\n",
    "\n",
    "# Invoke the chain\n",
    "result_list = list_chain.invoke({\"item_type\": \"unique ai models\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "08708257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Output\n",
      "['GPT-3', 'BERT', 'DALL-E']\n",
      "Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"List Output\")\n",
    "print(result_list)\n",
    "print(f\"Type: {type(result_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90918fe",
   "metadata": {},
   "source": [
    "`Without get_format_instructions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c12d08f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4', 'DALL-E 2', 'BERT']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Use the JSON Output Parser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Define your prompt template\n",
    "template = \"List three {item_type}. Provide in comma separated values.\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Parse the LLM response\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"item_type\": \"unique ai models\"})\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfe7cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3. JSON Output Parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a16a2",
   "metadata": {},
   "source": [
    "`With get_format_instructions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cba9a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a JSON object.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Use the JSON Output Parser\n",
    "parser = JsonOutputParser()\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e3426de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Doe', 'age': 30}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Define your prompt template\n",
    "template = \"Extract the name and age from this text: {text}\\n\\n{format_instructions}\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Parse the LLM response\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"text\": \"John Doe is 30 years old.\"})\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78968037",
   "metadata": {},
   "source": [
    "`Without get_format_instructions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "895a56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Doe', 'age': 30}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Use the JSON Output Parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Define your prompt template\n",
    "template = \"Extract the name and age from this text: {text}. Provide json output.\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Parse the LLM response\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"text\": \"John Doe is 30 years old.\"})\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08404802",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 4. Pydantic Output Parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48495d6",
   "metadata": {},
   "source": [
    "`What is Pydantic?`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "31a4ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Laptop(BaseModel):\n",
    "    name: str = Field(description=\"Provide the name of the Laptop\")\n",
    "    processor: str = Field(description=\"Provide the name of the processor used in the laptop\")\n",
    "    generation: int = Field(description=\"Provide the processor generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2db044d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Macbook Air' processor='M4' generation=4\n"
     ]
    }
   ],
   "source": [
    "laptop_1 = Laptop(name=\"Macbook Air\", processor=\"M4\", generation=4)\n",
    "print(laptop_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09ffeffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macbook Air\n",
      "M4\n"
     ]
    }
   ],
   "source": [
    "print(laptop_1.name)\n",
    "print(laptop_1.processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4063c89c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Laptop\ngeneration\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='5th', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[129]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m laptop_2 = \u001b[43mLaptop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMacbook Pro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mM5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m5th\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/youtube/langchain-youtube/.langchain-youtube-env/lib/python3.14/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Laptop\ngeneration\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='5th', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.12/v/int_parsing"
     ]
    }
   ],
   "source": [
    "laptop_2 = Laptop(name=\"Macbook Pro\", processor=\"M5\", generation=\"5th\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265399b6",
   "metadata": {},
   "source": [
    "`Json Output Parser with Pydantic Class`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9d924d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRICT OUTPUT FORMAT:\n",
      "- Return only the JSON value that conforms to the schema. Do not include any additional text, explanations, headings, or separators.\n",
      "- Do not wrap the JSON in Markdown or code fences (no ``` or ```json).\n",
      "- Do not prepend or append any text (e.g., do not write \"Here is the JSON:\").\n",
      "- The response must be a single top-level JSON value exactly as required by the schema (object/array/etc.), with no trailing commas or comments.\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]} the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema (shown in a code block for readability only â€” do not include any backticks or Markdown in your output):\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"The person's name\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"description\": \"The person's age\", \"title\": \"Age\", \"type\": \"integer\"}}, \"required\": [\"name\", \"age\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Optional: Define a Pydantic model for structured output\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\")\n",
    "    age: int = Field(description=\"The person's age\")\n",
    "\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Use the JSON Output Parser with optional Pydantic model\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "# Get format instructions from the parser\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3db4c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Doe', 'age': 30}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Define your prompt template\n",
    "template = \"\"\"Extract the name and age from this text: {text}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": format_instructions,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Invoke the chain\n",
    "response = chain.invoke({\"text\": \"John Doe is 30 years old.\"})\n",
    "\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503f27f",
   "metadata": {},
   "source": [
    "`Pydantic Output Parser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "75a58b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"The formal name of the recipe.\", \"title\": \"Name\", \"type\": \"string\"}, \"ingredients\": {\"description\": \"A list of 3 main ingredients.\", \"items\": {\"type\": \"string\"}, \"title\": \"Ingredients\", \"type\": \"array\"}, \"prep_time_minutes\": {\"description\": \"The preparation time in minutes.\", \"title\": \"Prep Time Minutes\", \"type\": \"integer\"}}, \"required\": [\"name\", \"ingredients\", \"prep_time_minutes\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# Define the exact structure we want the LLM to return\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"The formal name of the recipe.\")\n",
    "    ingredients: List[str] = Field(description=\"A list of 3 main ingredients.\")\n",
    "    prep_time_minutes: int = Field(description=\"The preparation time in minutes.\")\n",
    "\n",
    "\n",
    "# Instantiate the parser with the Pydantic model\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# Get the Pydantic JSON schema instructions\n",
    "pydantic_instructions = pydantic_parser.get_format_instructions()\n",
    "print(pydantic_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "741a7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt, injecting the instructions\n",
    "pydantic_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Create a simple recipe for {food_item}. \\n\\n{pydantic_instructions}\"\n",
    ")\n",
    "\n",
    "# Build the LCEL Chain\n",
    "pydantic_chain = pydantic_prompt.partial(pydantic_instructions=pydantic_instructions) | llm | pydantic_parser\n",
    "\n",
    "# Invoke the chain\n",
    "result_object: Recipe = pydantic_chain.invoke({\"food_item\": \"tacos\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44b3b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic Object Output\n",
      "Name: Simple Tacos\n",
      "Prep Time: 15 minutes\n",
      "First Ingredient: Taco shells\n",
      "Type: <class '__main__.Recipe'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pydantic Object Output\")\n",
    "print(f\"Name: {result_object.name}\")\n",
    "print(f\"Prep Time: {result_object.prep_time_minutes} minutes\")\n",
    "print(f\"First Ingredient: {result_object.ingredients[0]}\")\n",
    "print(f\"Type: {type(result_object)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dde05",
   "metadata": {},
   "source": [
    "`Another way to use Pydantic Output Parser`\n",
    "but it will only work with OpenAI models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2439224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic Object Output\n",
      "Name: Simple Beef Tacos\n",
      "Prep Time: 15 minutes\n",
      "First Ingredient: Ground beef\n",
      "Type: <class '__main__.Recipe'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# Define the exact structure we want the LLM to return\n",
    "class Recipe(BaseModel):\n",
    "    name: str = Field(description=\"The formal name of the recipe.\")\n",
    "    ingredients: List[str] = Field(description=\"A list of 3 main ingredients.\")\n",
    "    prep_time_minutes: int = Field(description=\"The preparation time in minutes.\")\n",
    "    number_of_people: int = Field(description=\"How many people required to cook?\")\n",
    "\n",
    "\n",
    "# Create the prompt, injecting the instructions\n",
    "pydantic_prompt = ChatPromptTemplate.from_template(\"Create a simple recipe for {food_item}.\")\n",
    "\n",
    "# LLM with structured output\n",
    "llm_structured = llm.with_structured_output(schema=Recipe)\n",
    "\n",
    "# Build the LCEL Chain\n",
    "pydantic_chain = pydantic_prompt | llm_structured\n",
    "\n",
    "# Invoke the chain\n",
    "result_object: Recipe = pydantic_chain.invoke({\"food_item\": \"tacos\"})\n",
    "\n",
    "print(f\"Pydantic Object Output\")\n",
    "print(f\"Name: {result_object.name}\")\n",
    "print(f\"Prep Time: {result_object.prep_time_minutes} minutes\")\n",
    "print(f\"First Ingredient: {result_object.ingredients[0]}\")\n",
    "print(f\"Type: {type(result_object)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d4c3556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_object.number_of_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0d486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
